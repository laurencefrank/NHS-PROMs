{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactored notebook for modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import re\n",
    "\n",
    "from NHS_PROMs.load_data import load_proms, structure_name\n",
    "from NHS_PROMs.preprocess import filter_in_range, filter_in_labels, method_delta\n",
    "from NHS_PROMs.utils import downcast, map_labels, fillna_categories, pd_fit_resample\n",
    "from NHS_PROMs.data_dictionary import meta_dict\n",
    "\n",
    "# use adjusted fillna which can cope with non-existing categories\n",
    "pd.core.frame.DataFrame.fillna = fillna_categories\n",
    "pd.core.frame.DataFrame.fillna = fillna_categories\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.compose import make_column_selector\n",
    "# from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer, make_column_selector\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import classification_report, balanced_accuracy_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from imblearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "# enable autodetect by using \"infer\" + the use of column names\n",
    "SMOTENC.fit_resample = pd_fit_resample(SMOTENC.fit_resample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data\n",
    "General approach is not DRY for the sake of availability of having knee and hip df's always at hand, but also keep it readable (script-wise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data + rename columns with structired name\n",
    "# df_knee_raw = load_proms(part=\"knee\").apply(downcast).rename(structure_name, axis=1)\n",
    "df_hip_raw = load_proms(part=\"hip\").apply(downcast).rename(structure_name, axis=1)\n",
    "\n",
    "# get meta data for each\n",
    "full_meta = {t + k: v for k, v in meta_dict.items() for t in [\"t0_\", \"t1_\"]}\n",
    "hip_meta = {k: v for k, v in full_meta.items() if k in df_hip_raw.columns}\n",
    "\n",
    "df_hip_raw.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## basic cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endings = (\n",
    "    \"code\",\n",
    "    \"procedure\",\n",
    "    \"revision_flag\",\n",
    "    \"assisted_by\",\n",
    "    \"profile\",\n",
    "    \"predicted\",\n",
    ")\n",
    "cols2drop = [c for c in df_hip_raw.columns if c.endswith(endings)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df_hip_clean = (\n",
    "    df_hip_raw.apply(lambda s: filter_in_range(s, **hip_meta[s.name]))\n",
    "    .apply(lambda s: filter_in_labels(s, **hip_meta[s.name]))\n",
    "    .apply(lambda s: map_labels(s, **hip_meta[s.name]))\n",
    "    .query(\"t0_revision_flag == 'no revision'\")\n",
    "    .drop(columns=cols2drop)\n",
    "    #     .replace(\"missing\", np.nan)\n",
    ")\n",
    "\n",
    "df_hip_clean.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train + test set\n",
    "# df_knee_seen = df_knee_clean.query(\"t0_year != '2019/20'\")\n",
    "# df_knee_unseen = df_knee_clean.query(\"t0_year == '2019/20'\")\n",
    "\n",
    "df_hip = df_hip_clean.query(\"t0_year != '2019/20'\")\n",
    "df_hip_unseen = df_hip_clean.query(\"t0_year == '2019/20'\")\n",
    "\n",
    "df_hip.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create delta dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_org = df_hip_seen.apply(\n",
    "#     lambda s: map_labels(s, backwards=True, **hip_meta[s.name])\n",
    "# ).apply(np.asarray)\n",
    "\n",
    "# # df_knee_delta = method_delta(df_knee_train)\n",
    "# df_hip_delta = method_delta(df_org)\n",
    "\n",
    "# # now you could join them again with the original df ...\n",
    "# # eg: df_hip_train.join(df_hip_delta)\n",
    "# df_hip_delta.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asses quickly missing\n",
    "print(len(df_hip), \"original\")\n",
    "print(len(df_hip.dropna()), \"after possible total dropna\")\n",
    "(df_hip.isna().sum() / len(df_hip)).sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove NaNs from non categorical/ordinal columns (numerical)\n",
    "print(len(df_hip), \"original\")\n",
    "num_cols = df_hip.select_dtypes(exclude=\"category\").columns\n",
    "df_hip = df_hip.dropna(subset=num_cols).fillna(value=\"missing\")\n",
    "\n",
    "print(len(df_hip), \"after dropna on numerical + fillna on categories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create x, y\n",
    "X = df_hip.filter(regex=\"t0\")\n",
    "y = (df_hip[\"t1_ohs_score\"] - df_hip[\"t0_ohs_score\"] <= 20).astype(int) # knee <= 7\n",
    "\n",
    "# make a smaller selection of our training data to play with\n",
    "X = X.iloc[:1000, -5:]\n",
    "y = y.iloc[:1000]\n",
    "\n",
    "\n",
    "# create train, test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"before:\")\n",
    "# display(y_train.value_counts())\n",
    "\n",
    "# # cat_cols = X_train.dtypes == \"category\"\n",
    "# # cat_cols = X_train.columns[cat_cols] TO DO: Fix with eg column selector\n",
    "\n",
    "# resampler = SMOTENC(categorical_features=\"infer\")\n",
    "# X_train_balanced, y_train_balanced = resampler.fit_resample(X_train, y_train)\n",
    "\n",
    "# print(\"after:\")\n",
    "# display(y_train_balanced.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make + train a simple pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the pipeline\n",
    "# ct = make_column_transformer(\n",
    "#     (OneHotEncoder(), make_column_selector(dtype_include=\"category\")),\n",
    "#     (StandardScaler(), make_column_selector(dtype_include=\"number\")),\n",
    "# )\n",
    "#\n",
    "# pl = make_pipeline(\n",
    "# #     SMOTENC(categorical_features=\"infer\"), # disabled just for the sake of speed\n",
    "#     ct,\n",
    "#     KNeighborsClassifier(),\n",
    "# )\n",
    "\n",
    "\n",
    "ct = ColumnTransformer(\n",
    "    (\n",
    "        (\"categorical\", OneHotEncoder(), make_column_selector(dtype_include=\"category\")),\n",
    "        (\"numerical\", StandardScaler(), make_column_selector(dtype_include=\"number\")),\n",
    "    ),\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "pl = Pipeline(\n",
    "    (\n",
    "        (\"balancer\", SMOTENC(categorical_features=\"infer\")),\n",
    "        (\"by_column_types\", ct),\n",
    "        (\"model\", KNeighborsClassifier()),\n",
    "    )\n",
    ")\n",
    "\n",
    "# train the pipeline/model\n",
    "pl.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict + evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prediction\n",
    "y_hat = pl.predict(X_test.head(500))\n",
    "\n",
    "# evaluate\n",
    "print(classification_report(y_test.head(500), y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create parameter grid to search on \n",
    "# standard (same as pipeline)\n",
    "param_grid = dict()\n",
    "\n",
    "# construct gridsearch\n",
    "GS = GridSearchCV(pl, param_grid=param_grid) ## add scoring\n",
    "\n",
    "# train gridsearch\n",
    "GS.fit(X_train, y_train)\n",
    "\n",
    "# show results\n",
    "pd.DataFrame(GS.cv_results_)\\\n",
    "    .filter(regex=r\"^(?!.*(split|time)).*$\")\\\n",
    "    .set_index(\"rank_test_score\").sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create parameter grid to search on \n",
    "\n",
    "# standard same as pipeline\n",
    "param_grid = dict()\n",
    "\n",
    "# # two models with default parameters\n",
    "# param_grid = {\"model\": [KNeighborsClassifier(), DecisionTreeClassifier()]}\n",
    "\n",
    "# # tuning hyper parameters\n",
    "# param_grid = {\n",
    "#     \"model\": [RandomForestClassifier(), AdaBoostClassifier()],\n",
    "#     \"model__n_estimators\": [25, 50, 100],\n",
    "# }\n",
    "\n",
    "# # tuning different hyper parameters on different models\n",
    "# param_grid = [\n",
    "#     {\n",
    "#         \"model\": [RandomForestClassifier()],\n",
    "#         \"model__n_estimators\": [25, 50, 100],\n",
    "#     },\n",
    "#     {\n",
    "#         \"model\": [KNeighborsClassifier()],\n",
    "#         \"model__n_neighbors\": [2, 5, 10],\n",
    "#     },\n",
    "# ]\n",
    "\n",
    "\n",
    "# construct gridsearch\n",
    "\n",
    "# standard\n",
    "GS = GridSearchCV(pl, param_grid=param_grid)\n",
    "\n",
    "# # add scoring \n",
    "# GS = GridSearchCV(pl, param_grid=param_grid, scoring=[\"f1\"])\n",
    "\n",
    "# # multiple\n",
    "# GS = GridSearchCV(pl, param_grid=param_grid, scoring=[\"balanced_accuracy\", \"f1\"], refit=False)\n",
    "\n",
    "\n",
    "# train gridsearch\n",
    "GS.fit(X_train, y_train)\n",
    "\n",
    "# show results\n",
    "pd.DataFrame(GS.cv_results_)\\\n",
    "    .filter(regex=r\"^(?!.*(split|time)).*$\")\n",
    "#     .set_index(\"rank_test_score\").sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create x, y\n",
    "X = df_hip.filter(regex=\"t0\")\n",
    "y = df_hip[\"t1_ohs_score\"] - df_hip[\"t0_ohs_score\"]\n",
    "\n",
    "\n",
    "# create train, test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42\n",
    ")\n",
    "\n",
    "# make a smaller selection of our training data to play with\n",
    "X_train = X_train.iloc[:1000, -5:]\n",
    "y_train = y_train.iloc[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make parameter grid\n",
    "param_grid = {\n",
    "    \"balancer\": [\"passthrough\"],\n",
    "    \"model\": [KNeighborsRegressor()],\n",
    "}\n",
    "\n",
    "GS = GridSearchCV(pl, param_grid=param_grid)\n",
    "# train gridsearch\n",
    "GS.fit(X_train, y_train)\n",
    "\n",
    "# show results\n",
    "pd.DataFrame(GS.cv_results_)\\\n",
    "    .filter(regex=r\"^(?!.*(split|time)).*$\")\n",
    "#     .set_index(\"rank_test_score\").sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract feature names pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the feature names from pipeline\n",
    "def get_feature_names(sklobj, feature_names=[]):\n",
    "\n",
    "    if isinstance(sklobj, Pipeline):\n",
    "        for name, step in sklobj.steps:\n",
    "            get_feature_names(step, feature_names)\n",
    "    elif isinstance(sklobj, ColumnTransformer):\n",
    "        for name, transformer, columns in sklobj.transformers_:\n",
    "            feature_names += get_feature_names(transformer, columns)\n",
    "    elif isinstance(sklobj, OneHotEncoder):\n",
    "        feature_names = sklobj.get_feature_names(feature_names).tolist()\n",
    "    elif isinstance(sklobj, str):\n",
    "        if sklobj == \"passthrough\":\n",
    "            pass\n",
    "        elif sklobj == \"drop\":\n",
    "            feature_names = []\n",
    "            \n",
    "    return feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_feature_names(pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this is slow ...\n",
    "# r = permutation_importance(pl, X_train.head(1_000), y_train.head(1_000), n_repeats=2, random_state=0)\n",
    "\n",
    "# feature_names = get_feature_names(pl)\n",
    "\n",
    "# for i in r.importances_mean.argsort()[::-1]:\n",
    "#     if r.importances_mean[i] - 2 * r.importances_std[i] > 0:\n",
    "#         print(f\"{feature_names[i]:<8}\"\n",
    "#         f\"{r.importances_mean[i]:.3f}\"\n",
    "#         f\" +/- {r.importances_std[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a more advanced pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO ..."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:jads] *",
   "language": "python",
   "name": "conda-env-jads-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
