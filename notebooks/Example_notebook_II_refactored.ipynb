{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactored notebook for modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import re\n",
    "\n",
    "from NHS_PROMs.load_data import load_proms, structure_name\n",
    "from NHS_PROMs.preprocess import filter_in_range, filter_in_labels, method_delta\n",
    "from NHS_PROMs.utils import downcast, map_labels\n",
    "from NHS_PROMs.data_dictionary import meta_dict\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import make_column_selector\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer, make_column_selector\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "\n",
    "from imblearn.over_sampling import SMOTENC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data\n",
    "General approach is not DRY for the sake of availability of having knee and hip df's always at hand, but also keep it readable (script-wise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data + rename columns with structired name\n",
    "# df_knee_raw = load_proms(part=\"knee\").apply(downcast).rename(structure_name, axis=1)\n",
    "df_hip_raw = load_proms(part=\"hip\").apply(downcast).rename(structure_name, axis=1)\n",
    "\n",
    "# get meta data for each\n",
    "full_meta = {t + k: v for k, v in meta_dict.items() for t in [\"t0_\", \"t1_\"]}\n",
    "hip_meta = {k: v for k, v in full_meta.items() if k in df_hip_raw.columns}\n",
    "\n",
    "df_hip_raw.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## basic cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endings = (\n",
    "    \"code\",\n",
    "    \"procedure\",\n",
    "    \"revision_flag\",\n",
    "    \"assisted_by\",\n",
    "    \"profile\",\n",
    "    \"predicted\",\n",
    ")\n",
    "cols2drop = [c for c in df_hip_raw.columns if c.endswith(endings)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df_hip_clean = (\n",
    "    df_hip_raw.apply(lambda s: filter_in_range(s, **hip_meta[s.name]))\n",
    "    .apply(lambda s: filter_in_labels(s, **hip_meta[s.name]))\n",
    "    .apply(lambda s: map_labels(s, **hip_meta[s.name]))\n",
    "    .query(\"t0_revision_flag == 'no revision'\")\n",
    "    .drop(columns=cols2drop)\n",
    "    #     .replace(\"missing\", np.nan)\n",
    ")\n",
    "\n",
    "df_hip_clean.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train + test set\n",
    "# df_knee_seen = df_knee_clean.query(\"t0_year != '2019/20'\")\n",
    "# df_knee_unseen = df_knee_clean.query(\"t0_year == '2019/20'\")\n",
    "\n",
    "df_hip = df_hip_clean.query(\"t0_year != '2019/20'\")\n",
    "df_hip_unseen = df_hip_clean.query(\"t0_year == '2019/20'\")\n",
    "\n",
    "df_hip.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create delta dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_org = df_hip_seen.apply(\n",
    "#     lambda s: map_labels(s, backwards=True, **hip_meta[s.name])\n",
    "# ).apply(np.asarray)\n",
    "\n",
    "# # df_knee_delta = method_delta(df_knee_train)\n",
    "# df_hip_delta = method_delta(df_org)\n",
    "\n",
    "# # now you could join them again with the original df ...\n",
    "# # eg: df_hip_train.join(df_hip_delta)\n",
    "# df_hip_delta.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_hip), \"original\")\n",
    "print(len(df_hip.dropna()), \"after possible total dropna\")\n",
    "(df_hip.isna().sum() / len(df_hip)).sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove NaNs from non categorical/ordinal columns (numerical)\n",
    "print(len(df_hip), \"original\")\n",
    "num_cols = df_hip.select_dtypes(exclude=\"category\").columns\n",
    "df_hip = df_hip.dropna(subset=num_cols)\n",
    "\n",
    "print(len(df_hip), \"after dropna on numerical\")\n",
    "(df_hip.isna().sum() / len(df_hip)).sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Buggy: TO DO ...\n",
    "# s = df_hip[\"t0_age_band\"]\n",
    "\n",
    "# def fillna(s, value):\n",
    "#     if s.dtype == \"category\":\n",
    "#         print(\"is cat\")\n",
    "#         if value not in s.dtype.categories:\n",
    "#             print(value, \"not in cats\")\n",
    "#             display(s.dtype)\n",
    "# #             display(s.dtype.add_categories(value))\n",
    "#             s.add_categories(value, inplace=True)\n",
    "#             display(s.dtype)\n",
    "#     return s.fillna(value)\n",
    "\n",
    "# fillna(s, \"missing\")\n",
    "\n",
    "# # status before\n",
    "# display((df_hip.isna().sum() / len(df_hip)).sort_values(ascending=False).head(10))\n",
    "# print(len(df_hip), \"original\")\n",
    "\n",
    "# # remove NaNs from non categorical/ordinal columns (numerical)\n",
    "# num_cols = df_hip.select_dtypes(include=\"number\").columns\n",
    "# df_hip = df_hip.dropna(subset=num_cols).fillna(\"missing\")\n",
    "# print(len(df_hip), \"after dropna on numerical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hip = df_hip.dropna().sample(10_000) # dirty drop for the sake of testing pl\n",
    "\n",
    "# create x, y\n",
    "X = df_hip.filter(regex=\"t0\")\n",
    "y = (df_hip[\"t1_ohs_score\"] - df_hip[\"t0_ohs_score\"] <= 8).astype(int) # knee <= 7\n",
    "\n",
    "\n",
    "# create train, test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## show OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example to show it works\n",
    "OHE = OneHotEncoder(sparse=False, dtype=\"uint8\")\n",
    "OHE.fit(X_train)\n",
    "\n",
    "pd.DataFrame(\n",
    "    data=OHE.transform(X_train),\n",
    "    index=X_train.index,\n",
    "    columns=OHE.get_feature_names(X_train.columns),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"before:\")\n",
    "display(y_train.value_counts())\n",
    "\n",
    "cat_cols = X_train.dtypes == \"category\"\n",
    "\n",
    "resampler = SMOTENC(categorical_features=cat_cols.values)\n",
    "X_train_balanced, y_train_balanced = resampler.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"after:\")\n",
    "display(y_train_balanced.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make + train a simple pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the pipeline\n",
    "ct = make_column_transformer(\n",
    "    (OneHotEncoder(), make_column_selector(dtype_include=\"category\")),\n",
    "    (StandardScaler(), make_column_selector(dtype_include=\"number\")),\n",
    ") \n",
    "\n",
    "pl = make_pipeline(ct, KNeighborsClassifier())\n",
    "\n",
    "# train the pipeline/model\n",
    "pl.fit(X_train_balanced, y_train_balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict + evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prediction\n",
    "y_hat = pl.predict(X_test.head(500))\n",
    "\n",
    "# evaluate\n",
    "print(classification_report(y_test.head(500), y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract feature names pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the feature names from pipeline\n",
    "def get_feature_names(sklobj, feature_names=[]):\n",
    "\n",
    "    if isinstance(sklobj, Pipeline):\n",
    "        for name, step in sklobj.steps:\n",
    "            get_feature_names(step, feature_names)\n",
    "    elif isinstance(sklobj, ColumnTransformer):\n",
    "        for name, transformer, columns in sklobj.transformers_:\n",
    "            feature_names += get_feature_names(transformer, columns)\n",
    "    elif isinstance(sklobj, OneHotEncoder):\n",
    "        feature_names = sklobj.get_feature_names(feature_names).tolist()\n",
    "    elif isinstance(sklobj, str):\n",
    "        if sklobj == \"passthrough\":\n",
    "            pass\n",
    "        elif sklobj == \"drop\":\n",
    "            feature_names = []\n",
    "            \n",
    "    return feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "get_feature_names(pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this is slow ...\n",
    "# r = permutation_importance(pl, X_train.head(1_000), y_train.head(1_000), n_repeats=2, random_state=0)\n",
    "\n",
    "# feature_names = get_feature_names(pl)\n",
    "\n",
    "# for i in r.importances_mean.argsort()[::-1]:\n",
    "#     if r.importances_mean[i] - 2 * r.importances_std[i] > 0:\n",
    "#         print(f\"{feature_names[i]:<8}\"\n",
    "#         f\"{r.importances_mean[i]:.3f}\"\n",
    "#         f\" +/- {r.importances_std[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a more advanced pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## debug issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(\"abcebdebca\")\n",
    "categories = list(\"abcd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = pd.Categorical(data, categories=categories, ordered=True)\n",
    "s1 = pd.Series(cats)\n",
    "\n",
    "display(s1)\n",
    "s1.cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.api.types import CategoricalDtype\n",
    "cat_type = CategoricalDtype(categories=categories, ordered=True)\n",
    "s2 = pd.Series(data).astype(cat_type)\n",
    "\n",
    "display(s2)\n",
    "s2.cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = pd.Series(data).astype(\"category\")\n",
    "\n",
    "display(s3)\n",
    "s3.cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:jads]",
   "language": "python",
   "name": "conda-env-jads-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
