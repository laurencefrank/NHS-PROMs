{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Refactored notebook for modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import warnings\n",
    "import re\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from NHS_PROMs.settings import config\n",
    "from NHS_PROMs.load_data import load_proms, structure_name\n",
    "from NHS_PROMs.preprocess import filter_in_range, filter_in_labels, method_delta\n",
    "from NHS_PROMs.utils import (\n",
    "    most_recent_file,\n",
    "    downcast,\n",
    "    map_labels,\n",
    "    fillna_categories,\n",
    "    pd_fit_resample,\n",
    "    infer_categories_fit,\n",
    "    KindSelector,\n",
    "    get_feature_names,\n",
    "    remove_categories,\n",
    ")\n",
    "from NHS_PROMs.data_dictionary import meta_dict, methods\n",
    "\n",
    "import shap\n",
    "shap.initjs()\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.compose import make_column_selector\n",
    "\n",
    "# from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import (\n",
    "    ColumnTransformer,\n",
    "    make_column_transformer,\n",
    "    make_column_selector,\n",
    ")\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    GradientBoostingRegressor,\n",
    "    BaggingClassifier,\n",
    ")\n",
    "from sklearn.metrics import classification_report, balanced_accuracy_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn import set_config\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "set_config(display=\"diagram\")\n",
    "\n",
    "from imblearn.ensemble import BalancedBaggingClassifier, BalancedRandomForestClassifier, EasyEnsembleClassifier\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from imblearn.pipeline import Pipeline, make_pipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler \n",
    "\n",
    "# use adjusted fillna which can cope with non-existing categories for CategoricalDtype\n",
    "pd.core.frame.DataFrame.fillna = fillna_categories\n",
    "# added a remove categories\n",
    "pd.core.frame.Series.remove_categories = remove_categories\n",
    "# enable autodetect of categories from CategoricalDtype by using \"infer\" for SMOTENC\n",
    "SMOTENC.fit_resample = pd_fit_resample(SMOTENC.fit_resample)\n",
    "# enable inference of categories for encoders from CategoricalDtype\n",
    "OneHotEncoder.fit = infer_categories_fit(OneHotEncoder.fit)\n",
    "OrdinalEncoder.fit = infer_categories_fit(OrdinalEncoder.fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## load data\n",
    "General approach is not DRY for the sake of availability of having knee and hip df's always at hand, but also keep it readable (script-wise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NHS_PROMs.model import pl, param_grid\n",
    "\n",
    "class PROMsModel():\n",
    "    def __init__(self, kind=\"hip\"):\n",
    "        self.kind = kind\n",
    "        self.outputs = config[\"outputs\"][kind]\n",
    "        \n",
    "    def load_data(self, mode=\"train\"):\n",
    "        df = (\n",
    "            load_proms(part=self.kind)\n",
    "            .apply(downcast)\n",
    "            .rename(structure_name, axis=1)\n",
    "        )\n",
    "        \n",
    "        self.load_meta(df.columns)\n",
    "        \n",
    "        df = self.preprocess(df)\n",
    "        \n",
    "        if mode==\"train\":\n",
    "            df = df.query(\"t0_year != 'April 2019 - April 2020'\").drop(columns=\"t0_year\")\n",
    "        elif mode==\"test\":\n",
    "            df = df.query(\"t0_year == 'April 2019 - April 2020'\").drop(columns=\"t0_year\")\n",
    "        else: \n",
    "            raise ValueError(f\"No valid mode: '{mode}'\")\n",
    "            \n",
    "        return df\n",
    "    \n",
    "    def load_meta(self, columns):\n",
    "        # get meta data \n",
    "        full_meta = {t + k: v for k, v in meta_dict.items() for t in [\"t0_\", \"t1_\"]}\n",
    "        self.meta = {k: v for k, v in full_meta.items() if k in columns}\n",
    "    \n",
    "    def preprocess(self, df):\n",
    "        # remove certain columns\n",
    "        endings = config[\"preprocessing\"][\"remove_columns_ending_with\"]\n",
    "        cols2drop = [c for c in df.columns if c.endswith(endings)]\n",
    "        \n",
    "        df = (\n",
    "            df.apply(lambda s: filter_in_range(s, **self.meta[s.name])) # filter in range numeric features\n",
    "            .apply(lambda s: filter_in_labels(s, **self.meta[s.name])) # filter in labels categorical features + ordinal if ordered\n",
    "            .apply(lambda s: map_labels(s, **self.meta[s.name])) # map the labels as values for readability\n",
    "            .query(\"t0_revision_flag == 'no revision'\") # drop revision cases\n",
    "            .drop(columns=cols2drop) # drop not needed columns\n",
    "        )\n",
    "\n",
    "        # remove low info values from columns (almost redundant) values\n",
    "        for col, value in config[\"preprocessing\"][\"remove_low_info_categories\"].items():\n",
    "            df[col] = df[col].remove_categories(value)\n",
    "        \n",
    "        # remove NaNs/missing/unknown from numerical and ordinal features\n",
    "        df = (\n",
    "            df.apply(pd.Series.remove_categories, args=([\"missing\", \"not known\"],))\n",
    "            .dropna(subset= KindSelector(kind=\"numerical\")(df) + KindSelector(kind=\"ordinal\")(df))\n",
    "            .reset_index(drop=True) # make index unique (prevent blow ups when joining)\n",
    "        )\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    def split_XY(self, df):\n",
    "        \n",
    "        # define inputs and outputs \n",
    "        X = df.filter(regex=\"t0\").copy()\n",
    "        Y = df[self.outputs].copy()\n",
    "        \n",
    "        # get cut from settings\n",
    "        for col in Y.columns:\n",
    "            if pd.api.types.is_numeric_dtype(Y[col]):\n",
    "                Y[col] = pd.cut(\n",
    "                    Y[col],\n",
    "                    include_lowest=True,\n",
    "                    **self.outputs[col],\n",
    "                )\n",
    "        \n",
    "        return X, Y\n",
    "\n",
    "    def train_models(self):\n",
    "        X, Y = (\n",
    "            self.load_data(mode=\"train\")\n",
    "            .pipe(self.split_XY)\n",
    "        )\n",
    "        self.models = dict()\n",
    "        for col, y in Y.iteritems():\n",
    "            self.models[col] = self.train_model(X, y)\n",
    "        return self\n",
    "        \n",
    "    def train_model(self, X, y):\n",
    "        GS = GridSearchCV(\n",
    "            estimator=pl,\n",
    "            param_grid=param_grid,\n",
    "            scoring=config[\"score\"]\n",
    "        )\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\")\n",
    "            GS.fit(X, y)\n",
    "        return GS\n",
    "    \n",
    "    def save_models(self, filename=None):\n",
    "        if filename is None:\n",
    "            hashable = frozenset(self.models.items())\n",
    "            sha = hex(hash(hashable))[-5:]\n",
    "            path = os.path.join(\"..\", config[\"models\"][\"path\"])\n",
    "            filename = f\"{self.kind}_{sha}.mdl\"\n",
    "        pickle.dump(self.models, open(os.path.join(path, filename), 'wb'))\n",
    "        \n",
    "    def load_models(self, filename=None):\n",
    "        path = os.path.join(\"..\", config[\"models\"][\"path\"])\n",
    "        if filename is None:\n",
    "            filename = most_recent_file(path, ext=\".mdl\", prefix=self.kind)\n",
    "            if filename is None:\n",
    "                raise ValueError(\"No correct models found!\")\n",
    "        else:\n",
    "            if not re.search(fr\"^{self.kind}_\", filename):\n",
    "                raise Warning(f\"File '{filename} does not seem to be having models for {self.kind}\")\n",
    "        self.models = pickle.load(open(os.path.join(path, filename), 'rb'))\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        y_hat = dict()\n",
    "        for name, model in self.models.items():\n",
    "            check_is_fitted(model)\n",
    "            y_hat[name] = model.predict(X)\n",
    "        return y_hat\n",
    "            \n",
    "    def predict_proba(self, X):\n",
    "        y_hat = dict()\n",
    "        for name, model in self.models.items():\n",
    "            check_is_fitted(model)\n",
    "            y_hat[name] = model.predict_proba(X)\n",
    "        return y_hat\n",
    "    \n",
    "    def labels_encoded(self):\n",
    "        y_labels = dict()\n",
    "        for name, model in self.models.items():\n",
    "            check_is_fitted(model)\n",
    "            y_labels[name] = model.classes_\n",
    "        return y_labels\n",
    "    \n",
    "    def classification_reports(self):\n",
    "        data = self.load_data(mode=\"test\")\n",
    "        X, Y = self.split_XY(data)\n",
    "        for name, model in self.models.items():\n",
    "            check_is_fitted(model)\n",
    "            y_hat = model.predict(X)\n",
    "            print(f\"\\nClassification report for {name}:\\n\")\n",
    "            print(classification_report(Y[name], y_hat))\n",
    "            \n",
    "    def get_explainer(self, name):\n",
    "        if hasattr(self, \"explainers\") is False:\n",
    "            self.explainers = dict()\n",
    "            \n",
    "        if self.explainers.get(name) is None:\n",
    "            model = self.models[name]\n",
    "            check_is_fitted(model)\n",
    "            self.explainers[name] = shap.TreeExplainer(\n",
    "                model.best_estimator_.named_steps[\"model\"],\n",
    "#                 feature_perturbation='interventional',\n",
    "#                 model_output=\"probability\",\n",
    "#                 data=self.load_data(\"train\"),\n",
    "            )\n",
    "        return self.explainers[name]\n",
    "        \n",
    "                \n",
    "    def force_plot(self, X, name):\n",
    "\n",
    "        if X.shape[0] != 1:\n",
    "            raise ValueError(\"First dimension should be 1. Expected a single case for force plot!\")\n",
    "        \n",
    "        model = self.models[name]\n",
    "        check_is_fitted(model)\n",
    "        explainer = self.get_explainer(name)\n",
    "        \n",
    "        # rescaleing base values for multiclass https://evgenypogorelov.com/multiclass-xgb-shap.html\n",
    "        def logodds_to_proba(logodds):\n",
    "            return np.exp(logodds)/np.exp(logodds).sum()\n",
    "        \n",
    "        predict_proba = model.predict_proba(X)\n",
    "        i_max = np.argmax(predict_proba)\n",
    "        end_value = predict_proba[0, i_max]\n",
    "        X_pre = model.best_estimator_[:-1].transform(X)\n",
    "        shap_values = explainer.shap_values(X_pre)[i_max]\n",
    "        base_value = logodds_to_proba(explainer.expected_value)[i_max]\n",
    "        feature_names = [re.sub(\"(t0_|gender_|_yes|_no|)\", \"\", n).replace(\"_\", \" \") for n in get_feature_names(model)]\n",
    "        out_names = f\"{name} = {self.labels_encoded()[name][i_max]}\"\n",
    "        \n",
    "        # rescaling according to https://github.com/slundberg/shap/issues/29\n",
    "        shap_values = shap_values / shap_values.sum() * (end_value - base_value)\n",
    "        \n",
    "        fp = shap.force_plot(\n",
    "            base_value=base_value, \n",
    "            shap_values=shap_values,\n",
    "#             features=X_pre,\n",
    "            feature_names=feature_names,\n",
    "            out_names=out_names,\n",
    "#             link=\"logit\",\n",
    "        )\n",
    "        return fp\n",
    "    \n",
    "    def force_plots(self, X=None):\n",
    "        if X is None:\n",
    "            df_data = PM.load_data(\"test\").sample()\n",
    "            X, Y = PM.split_XY(df_data)\n",
    "        \n",
    "        for name in self.outputs:\n",
    "            display(self.force_plot(X, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PM = PROMsModel(kind=\"hip\").train_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PM.force_plots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:jads] *",
   "language": "python",
   "name": "conda-env-jads-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
