{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Refactored notebook for modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import re\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from NHS_PROMs.load_data import load_proms, structure_name\n",
    "from NHS_PROMs.preprocess import filter_in_range, filter_in_labels, method_delta\n",
    "from NHS_PROMs.utils import downcast, map_labels, fillna_categories, pd_fit_resample\n",
    "from NHS_PROMs.data_dictionary import meta_dict\n",
    "\n",
    "# use adjusted fillna which can cope with non-existing categories\n",
    "pd.core.frame.DataFrame.fillna = fillna_categories\n",
    "pd.core.frame.DataFrame.fillna = fillna_categories\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.compose import make_column_selector\n",
    "# from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer, make_column_selector\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingRegressor\n",
    "from sklearn.metrics import classification_report, balanced_accuracy_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from imblearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "# enable autodetect by using \"infer\" + the use of column names\n",
    "SMOTENC.fit_resample = pd_fit_resample(SMOTENC.fit_resample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## load data\n",
    "General approach is not DRY for the sake of availability of having knee and hip df's always at hand, but also keep it readable (script-wise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# load data + rename columns with structired name\n",
    "# df_knee_raw = load_proms(part=\"knee\").apply(downcast).rename(structure_name, axis=1)\n",
    "df_hip_raw = load_proms(part=\"hip\").apply(downcast).rename(structure_name, axis=1)\n",
    "\n",
    "# get meta data for each\n",
    "full_meta = {t + k: v for k, v in meta_dict.items() for t in [\"t0_\", \"t1_\"]}\n",
    "hip_meta = {k: v for k, v in full_meta.items() if k in df_hip_raw.columns}\n",
    "\n",
    "df_hip_raw.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## basic cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "endings = (\n",
    "    \"code\",\n",
    "    \"procedure\",\n",
    "    \"revision_flag\",\n",
    "    \"assisted_by\",\n",
    "    \"profile\",\n",
    "    \"predicted\",\n",
    ")\n",
    "cols2drop = [c for c in df_hip_raw.columns if c.endswith(endings)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df_hip_clean = (\n",
    "    df_hip_raw.apply(lambda s: filter_in_range(s, **hip_meta[s.name]))\n",
    "    .apply(lambda s: filter_in_labels(s, **hip_meta[s.name]))\n",
    "    .apply(lambda s: map_labels(s, **hip_meta[s.name]))\n",
    "    .query(\"t0_revision_flag == 'no revision'\")\n",
    "    .drop(columns=cols2drop)\n",
    "    .reset_index(drop=True)\n",
    "    #     .replace(\"missing\", np.nan)\n",
    ")\n",
    "\n",
    "df_hip_clean.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# split train + test set\n",
    "# df_knee_seen = df_knee_clean.query(\"t0_year != '2019/20'\")\n",
    "# df_knee_unseen = df_knee_clean.query(\"t0_year == '2019/20'\")\n",
    "\n",
    "df_hip = df_hip_clean.query(\"t0_year != '2019/20'\")\n",
    "df_hip_unseen = df_hip_clean.query(\"t0_year == '2019/20'\")\n",
    "\n",
    "df_hip.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## make feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# asses quickly missing\n",
    "print(len(df_hip), \"original\")\n",
    "print(len(df_hip.dropna()), \"after possible total dropna\")\n",
    "(df_hip.isna().sum() / len(df_hip)).sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# remove NaNs from non categorical/ordinal columns (numerical)\n",
    "print(len(df_hip), \"original\")\n",
    "num_cols = df_hip.select_dtypes(exclude=\"category\").columns\n",
    "df_hip = df_hip.dropna(subset=num_cols).fillna(value=\"missing\")\n",
    "\n",
    "print(len(df_hip), \"after dropna on numerical + fillna on categories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# create x, y\n",
    "X = df_hip.filter(regex=\"t0\")\n",
    "y = df_hip[\"t1_ohs_score\"] - df_hip[\"t0_ohs_score\"]\n",
    "\n",
    "# make a smaller selection of our training data to play with\n",
    "X = X.iloc[:1000, -5:]\n",
    "y = y.iloc[:1000]\n",
    "\n",
    "\n",
    "# create train, test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## make + train a simple pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "ct = ColumnTransformer(\n",
    "    (\n",
    "        (\"categorical\", OneHotEncoder(), make_column_selector(dtype_include=\"category\")),\n",
    "        (\"numerical\", StandardScaler(), make_column_selector(dtype_include=\"number\")),\n",
    "    ),\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "pl = Pipeline(\n",
    "    (\n",
    "        (\"by_column_types\", ct),\n",
    "        (\"model\", GradientBoostingRegressor()),\n",
    "    )\n",
    ")\n",
    "\n",
    "# train the pipeline/model\n",
    "pl.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## predict + evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# # make prediction\n",
    "# y_hat = pl.predict(X_test.head(500))\n",
    "\n",
    "# # evaluate\n",
    "# print(classification_report(y_test.head(500), y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## prediction intervals\n",
    "Last time we were talking about confidence intervals.\n",
    "\n",
    "But we assumed that for individual prediction we are meaning prediction intervals, correct?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## used sources\n",
    "basic explaination:\n",
    "* https://machinelearningmastery.com/prediction-intervals-for-machine-learning/\n",
    "* https://towardsdatascience.com/quantile-regression-from-linear-models-to-trees-to-deep-learning-af3738b527c3\n",
    "\n",
    "using parallel models with the quantile loss function for gradient boosting model:\n",
    "* https://towardsdatascience.com/how-confidence-and-prediction-intervals-work-4592019576d8\n",
    "* https://towardsdatascience.com/how-to-generate-prediction-intervals-with-scikit-learn-and-python-ab3899f992ed\n",
    "* https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0\n",
    "\n",
    "linear regression approach:\n",
    "* https://towardsdatascience.com/prediction-intervals-in-linear-regression-2ea14d419981\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Our current worked out example is based on:\n",
    "* parallel gradient boosting models \n",
    "* using different quantile loss function alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "pl.named_steps[\"model\"].set_params(loss=\"quantile\", alpha=0.9)\n",
    "pl.fit(X_train, y_train)\n",
    "y_90 = pl.predict(X_test)\n",
    "\n",
    "pl.named_steps[\"model\"].set_params(loss=\"quantile\", alpha=0.1)\n",
    "pl.fit(X_train, y_train)\n",
    "y_10 = pl.predict(X_test)\n",
    "\n",
    "pl.named_steps[\"model\"].set_params(loss=\"quantile\", alpha=0.5)\n",
    "pl.fit(X_train, y_train)\n",
    "y_hat = pl.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Plot of prediction intervals on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pd_conf = pd.DataFrame({\n",
    "    \"10%\": y_10,\n",
    "    \"90%\": y_90,\n",
    "    \"true\":y_test,\n",
    "    \"predicted\": y_hat,\n",
    "}).reset_index(drop=True)\n",
    "               \n",
    "# px.scatter(pd_conf)\n",
    "px.scatter(pd_conf, x=\"true\", y=\"predicted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## now do it smart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel\n",
    "from sklearn.multioutput import MultiOutputRegressor, _fit_estimator\n",
    "from sklearn.base import is_classifier\n",
    "from sklearn.utils.validation import _check_fit_params\n",
    "from sklearn.utils.fixes import delayed\n",
    "\n",
    "\n",
    "class ConfidenceEstimator(MultiOutputRegressor):\n",
    "    def __init__(self, estimator, quantiles, *, n_jobs=None):\n",
    "\n",
    "        super().__init__(estimator, n_jobs=n_jobs)\n",
    "        self.quantiles = quantiles\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None, **fit_params):\n",
    "        \"\"\"Fit the model to data.\n",
    "        Fit a separate model for each output variable.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            Data.\n",
    "        y : {array-like, sparse matrix} of shape (n_samples, n_outputs)\n",
    "            Multi-output targets. An indicator matrix turns on multilabel\n",
    "            estimation.\n",
    "        sample_weight : array-like of shape (n_samples,), default=None\n",
    "            Sample weights. If None, then samples are equally weighted.\n",
    "            Only supported if the underlying regressor supports sample\n",
    "            weights.\n",
    "        **fit_params : dict of string -> object\n",
    "            Parameters passed to the ``estimator.fit`` method of each step.\n",
    "            .. versionadded:: 0.23\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "        \"\"\"\n",
    "\n",
    "        if not hasattr(self.estimator, \"fit\"):\n",
    "            raise ValueError(\"The base estimator should implement\" \" a fit method\")\n",
    "\n",
    "        X, y = self._validate_data(\n",
    "            X, y, force_all_finite=False, multi_output=False, accept_sparse=True\n",
    "        )\n",
    "\n",
    "        if is_classifier(self):\n",
    "            check_classification_targets(y)\n",
    "\n",
    "        if sample_weight is not None and not has_fit_parameter(\n",
    "            self.estimator, \"sample_weight\"\n",
    "        ):\n",
    "            raise ValueError(\"Underlying estimator does not support\" \" sample weights.\")\n",
    "\n",
    "        fit_params_validated = _check_fit_params(X, fit_params)\n",
    "\n",
    "        self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
    "            delayed(_fit_estimator)(\n",
    "                self.estimator.set_params(loss=\"quantile\", alpha=alpha),\n",
    "                X,\n",
    "                y,\n",
    "                sample_weight,\n",
    "                **fit_params_validated\n",
    "            )\n",
    "            for alpha in self.quantiles\n",
    "        )\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "quantiles = [0.1, 0.5, 0.9]\n",
    "\n",
    "pl = Pipeline((\n",
    "    (\"by_column_types\", ct),\n",
    "    (\"model\", ConfidenceEstimator(GradientBoostingRegressor(), quantiles=quantiles)),\n",
    "))\n",
    "\n",
    "# train the pipeline/model\n",
    "pl.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "pl.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "X_i = X_test.sample()\n",
    "X_i.index[0]\n",
    "y_i = y_test.loc[X_i.index[0]]\n",
    "display(pl.predict(X_i), y_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## now do it over the top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## refactored it in a transfomer\n",
    "(still parallel models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# set confidence intervals\n",
    "step_size = 0.05\n",
    "quantiles = np.arange(step_size, 1, step_size)\n",
    "\n",
    "# setup pipeline\n",
    "pl = Pipeline((\n",
    "    (\"by_column_types\", ct),\n",
    "    (\"model\", ConfidenceEstimator(GradientBoostingRegressor(), quantiles=quantiles)),\n",
    "))\n",
    "  \n",
    "# train pipeline/model\n",
    "pl.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Questions:\n",
    "* Is this (in this particular form/model) what was meant in the last expert session?\n",
    "* (Because we have parallel models?) sometimes strange issues?\n",
    "    * eg: interval boundary 65% < 50%!\n",
    "    \n",
    "    How usefull is this approach then?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "y_int = pl.predict(X_test)\n",
    "prediction_intervals = {k:v for k, v in zip(quantiles, y_int)}\n",
    "\n",
    "def plot_prediction(intervals, true_value):\n",
    "    fig = go.Figure()\n",
    "\n",
    "    point_estimate = prediction_intervals[.5]\n",
    "\n",
    "    for label, x in prediction_intervals.items():\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[x, point_estimate], y=[1, 1], \n",
    "                fill='tozeroy', mode=\"none\", \n",
    "                fillcolor='rgba(255,0,0,0.1)',\n",
    "                showlegend=False,\n",
    "            )\n",
    "        )\n",
    "        if label != .5:\n",
    "            fig.add_annotation(\n",
    "                x=x, y=label,\n",
    "                text=f\"{label*100:.0f}%\",\n",
    "                showarrow=False,\n",
    "            )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[point_estimate]*2, y=[0, 1],\n",
    "            mode=\"lines\", line={\"color\":\"red\"}, \n",
    "            name=\"point estimate\",\n",
    "        )\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[true_value]*2, y=[0, 1],\n",
    "            mode=\"lines\", line={\"color\":\"blue\"}, \n",
    "            name=\"true value\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    x = list(prediction_intervals.values())\n",
    "    x_range = [np.min(x), np.max(x)]\n",
    "    x_range = (x_range - np.mean(x_range)) * 1.1 + np.mean(x_range)\n",
    "    fig.update_xaxes(range=x_range)\n",
    "    fig.update_yaxes(visible=False, showticklabels=False)\n",
    "    fig.update_layout(height=400)\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# take one sample from test set\n",
    "X_i = X_test.sample()\n",
    "y_i = y_test.loc[X_i.index[0]]\n",
    "# predict including prediction intervals\n",
    "y_int = pl.predict(X_i)[0]\n",
    "\n",
    "# plot prediction intervals\n",
    "prediction_intervals = {k:v for k, v in zip(quantiles, y_int)}\n",
    "plot_prediction(intervals=prediction_intervals, true_value=y_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# import plotly.graph_objs as go\n",
    "# fig = go.Figure([\n",
    "#     go.Scatter(\n",
    "#         name='Prediction',\n",
    "#         x=,\n",
    "#         y=df['10 Min Sampled Avg'],\n",
    "#         mode='lines',\n",
    "#         line=dict(color='rgb(31, 119, 180)'),\n",
    "#     ),\n",
    "#     go.Scatter(\n",
    "#         name='Upper Bound',\n",
    "#         x=df['Time'],\n",
    "#         y=df['10 Min Sampled Avg']+df['10 Min Std Dev'],\n",
    "#         mode='lines',\n",
    "#         marker=dict(color=\"#444\"),\n",
    "#         line=dict(width=0),\n",
    "#         showlegend=False\n",
    "#     ),\n",
    "#     go.Scatter(\n",
    "#         name='Lower Bound',\n",
    "#         x=df['Time'],\n",
    "#         y=df['10 Min Sampled Avg']-df['10 Min Std Dev'],\n",
    "#         marker=dict(color=\"#444\"),\n",
    "#         line=dict(width=0),\n",
    "#         mode='lines',\n",
    "#         fillcolor='rgba(68, 68, 68, 0.3)',\n",
    "#         fill='tonexty',\n",
    "#         showlegend=False\n",
    "#     )\n",
    "# ])\n",
    "# fig.update_layout(\n",
    "#     yaxis_title='Wind speed (m/s)',\n",
    "#     title='Continuous, variable value error bars',\n",
    "#     hovermode=\"x\"\n",
    "# )\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# create parameter grid to search on \n",
    "# standard (same as pipeline)\n",
    "param_grid = dict()\n",
    "\n",
    "# construct gridsearch\n",
    "GS = GridSearchCV(pl, param_grid=param_grid, scoring=\"f1\") ## add scoring\n",
    "\n",
    "# train gridsearch\n",
    "GS.fit(X_train, y_train)\n",
    "\n",
    "# show results\n",
    "pd.DataFrame(GS.cv_results_)\\\n",
    "    .filter(regex=r\"^(?!.*(split|time)).*$\")\\\n",
    "    .set_index(\"rank_test_score\").sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# create parameter grid to search on \n",
    "\n",
    "# standard same as pipeline\n",
    "param_grid = dict()\n",
    "\n",
    "# # # two models with default parameters\n",
    "param_grid = {\"model\": [KNeighborsClassifier(), DecisionTreeClassifier()]}\n",
    "\n",
    "# # tuning hyper parameters\n",
    "param_grid = {\n",
    "    \"model\": [RandomForestClassifier(), AdaBoostClassifier()],\n",
    "    \"model__n_estimators\": [25, 50, 100],\n",
    "}\n",
    "\n",
    "# tuning different hyper parameters on different models\n",
    "param_grid = [\n",
    "    {\n",
    "        \"model\": [RandomForestClassifier()],\n",
    "        \"model__n_estimators\": [25, 50, 100],\n",
    "    },\n",
    "    {\n",
    "        \"model\": [KNeighborsClassifier()],\n",
    "        \"model__n_neighbors\": [2, 5, 10],\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "# # construct gridsearch\n",
    "\n",
    "# # standard\n",
    "# GS = GridSearchCV(pl, param_grid=param_grid)\n",
    "\n",
    "# # # # add scoring \n",
    "# GS = GridSearchCV(pl, param_grid=param_grid, scoring=\"f1\")\n",
    "\n",
    "# # multiple\n",
    "GS = GridSearchCV(pl, param_grid=param_grid, scoring=[\"balanced_accuracy\", \"f1\"], refit=False)\n",
    "\n",
    "\n",
    "# train gridsearch\n",
    "GS.fit(X_train, y_train)\n",
    "\n",
    "# show results\n",
    "pd.DataFrame(GS.cv_results_)\\\n",
    "    .filter(regex=r\"^(?!.*(split|time)).*$\")\\\n",
    "#     .set_index(\"rank_test_score\").sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# create x, y\n",
    "X = df_hip.filter(regex=\"t0\")\n",
    "y = df_hip[\"t1_ohs_score\"] - df_hip[\"t0_ohs_score\"]\n",
    "\n",
    "\n",
    "# create train, test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42\n",
    ")\n",
    "\n",
    "# make a smaller selection of our training data to play with\n",
    "X_train = X_train.iloc[:1000, -5:]\n",
    "y_train = y_train.iloc[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# make parameter grid\n",
    "param_grid = {\n",
    "    \"balancer\": [\"passthrough\"],\n",
    "    \"model\": [KNeighborsRegressor()],\n",
    "}\n",
    "\n",
    "GS = GridSearchCV(pl, param_grid=param_grid)\n",
    "# train gridsearch\n",
    "GS.fit(X_train, y_train)\n",
    "\n",
    "# show results\n",
    "pd.DataFrame(GS.cv_results_)\\\n",
    "    .filter(regex=r\"^(?!.*(split|time)).*$\")\n",
    "#     .set_index(\"rank_test_score\").sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## extract feature names pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# get the feature names from pipeline\n",
    "def get_feature_names(sklobj, feature_names=[]):\n",
    "\n",
    "    if isinstance(sklobj, Pipeline):\n",
    "        for name, step in sklobj.steps:\n",
    "            get_feature_names(step, feature_names)\n",
    "    elif isinstance(sklobj, ColumnTransformer):\n",
    "        for name, transformer, columns in sklobj.transformers_:\n",
    "            feature_names += get_feature_names(transformer, columns)\n",
    "    elif isinstance(sklobj, OneHotEncoder):\n",
    "        feature_names = sklobj.get_feature_names(feature_names).tolist()\n",
    "    elif isinstance(sklobj, str):\n",
    "        if sklobj == \"passthrough\":\n",
    "            pass\n",
    "        elif sklobj == \"drop\":\n",
    "            feature_names = []\n",
    "            \n",
    "    return feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "get_feature_names(pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# # this is slow ...\n",
    "# r = permutation_importance(pl, X_train.head(1_000), y_train.head(1_000), n_repeats=2, random_state=0)\n",
    "\n",
    "# feature_names = get_feature_names(pl)\n",
    "\n",
    "# for i in r.importances_mean.argsort()[::-1]:\n",
    "#     if r.importances_mean[i] - 2 * r.importances_std[i] > 0:\n",
    "#         print(f\"{feature_names[i]:<8}\"\n",
    "#         f\"{r.importances_mean[i]:.3f}\"\n",
    "#         f\" +/- {r.importances_std[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## a more advanced pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# TO DO ..."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:jads] *",
   "language": "python",
   "name": "conda-env-jads-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
