{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Refactored notebook for modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import warnings\n",
    "import re\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from NHS_PROMs.settings import config\n",
    "from NHS_PROMs.load_data import load_proms, structure_name\n",
    "from NHS_PROMs.preprocess import filter_in_range, filter_in_labels, method_delta\n",
    "from NHS_PROMs.utils import (\n",
    "    most_recent_file,\n",
    "    downcast,\n",
    "    map_labels,\n",
    "    fillna_categories,\n",
    "    pd_fit_resample,\n",
    "    infer_categories_fit,\n",
    "    KindSelector,\n",
    "    get_feature_names,\n",
    "    remove_categories,\n",
    ")\n",
    "from NHS_PROMs.data_dictionary import meta_dict, methods\n",
    "\n",
    "import shap\n",
    "shap.initjs()\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.compose import make_column_selector\n",
    "\n",
    "# from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import (\n",
    "    ColumnTransformer,\n",
    "    make_column_transformer,\n",
    "    make_column_selector,\n",
    ")\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    GradientBoostingRegressor,\n",
    "    BaggingClassifier,\n",
    ")\n",
    "from sklearn.metrics import classification_report, balanced_accuracy_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn import set_config\n",
    "\n",
    "set_config(display=\"diagram\")\n",
    "\n",
    "from imblearn.ensemble import BalancedBaggingClassifier, BalancedRandomForestClassifier, EasyEnsembleClassifier\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from imblearn.pipeline import Pipeline, make_pipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler \n",
    "\n",
    "# use adjusted fillna which can cope with non-existing categories for CategoricalDtype\n",
    "pd.core.frame.DataFrame.fillna = fillna_categories\n",
    "# added a remove categories\n",
    "pd.core.frame.Series.remove_categories = remove_categories\n",
    "# enable autodetect of categories from CategoricalDtype by using \"infer\" for SMOTENC\n",
    "SMOTENC.fit_resample = pd_fit_resample(SMOTENC.fit_resample)\n",
    "# enable inference of categories for encoders from CategoricalDtype\n",
    "OneHotEncoder.fit = infer_categories_fit(OneHotEncoder.fit)\n",
    "OrdinalEncoder.fit = infer_categories_fit(OrdinalEncoder.fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## load data\n",
    "General approach is not DRY for the sake of availability of having knee and hip df's always at hand, but also keep it readable (script-wise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NHS_PROMs.model import pl, param_grid\n",
    "\n",
    "class PROMsModel():\n",
    "    def __init__(self, kind=\"hip\"):\n",
    "        self.kind = kind\n",
    "        self.outputs = config[\"outputs\"][kind]\n",
    "        \n",
    "    def load_data(self, mode=\"train\"):\n",
    "        df = (\n",
    "            load_proms(part=self.kind)\n",
    "            .apply(downcast)\n",
    "            .rename(structure_name, axis=1)\n",
    "        )\n",
    "        \n",
    "        self.load_meta(df.columns)\n",
    "        \n",
    "        df = self.preprocess(df)\n",
    "        \n",
    "        if mode==\"train\":\n",
    "            df = df.query(\"t0_year != 'April 2019 - April 2020'\").drop(columns=\"t0_year\")\n",
    "        elif mode==\"predict\":\n",
    "            df = df.query(\"t0_year == 'April 2019 - April 2020'\").drop(columns=\"t0_year\")\n",
    "        else: \n",
    "            raise ValueError(f\"No valid mode: '{mode}'\")\n",
    "            \n",
    "        return df\n",
    "    \n",
    "    def load_meta(self, columns):\n",
    "        # get meta data \n",
    "        full_meta = {t + k: v for k, v in meta_dict.items() for t in [\"t0_\", \"t1_\"]}\n",
    "        self.meta = {k: v for k, v in full_meta.items() if k in columns}\n",
    "    \n",
    "    def preprocess(self, df):\n",
    "        # remove certain columns\n",
    "        endings = config[\"preprocessing\"][\"remove_columns_ending_with\"]\n",
    "        cols2drop = [c for c in df.columns if c.endswith(endings)]\n",
    "        \n",
    "        df = (\n",
    "            df.apply(lambda s: filter_in_range(s, **self.meta[s.name])) # filter in range numeric features\n",
    "            .apply(lambda s: filter_in_labels(s, **self.meta[s.name])) # filter in labels categorical features + ordinal if ordered\n",
    "            .apply(lambda s: map_labels(s, **self.meta[s.name])) # map the labels as values for readability\n",
    "            .query(\"t0_revision_flag == 'no revision'\") # drop revision cases\n",
    "            .drop(columns=cols2drop) # drop not needed columns\n",
    "            .reset_index(drop=True) # make index unique (prevent blow ups when joining)\n",
    "        )\n",
    "\n",
    "        # remove NaNs/missing/unknown from numerical and ordinal features\n",
    "        df = (\n",
    "            df.apply(pd.Series.remove_categories, args=([\"missing\", \"not known\"],))\n",
    "            .dropna(subset= KindSelector(kind=\"numerical\")(df) + KindSelector(kind=\"ordinal\")(df))\n",
    "        )\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    def split_XY(self, df):\n",
    "        \n",
    "        # define inputs and outputs \n",
    "        X = df.filter(regex=\"t0\").copy()\n",
    "        Y = df[self.outputs].copy()\n",
    "        \n",
    "        # get cut from settings\n",
    "        for col in Y.columns:\n",
    "            if pd.api.types.is_numeric_dtype(Y[col]):\n",
    "                Y[col] = pd.cut(\n",
    "                    Y[col],\n",
    "                    include_lowest=True,\n",
    "                    **self.outputs[col],\n",
    "                )\n",
    "        \n",
    "        return X, Y\n",
    "\n",
    "    def train_models(self):\n",
    "        X, Y = (\n",
    "            self.load_data(mode=\"train\")\n",
    "            .pipe(self.split_XY)\n",
    "        )\n",
    "        self.models = dict()\n",
    "        for col, y in Y.iteritems():\n",
    "            self.models[col] = self.train_model(X, y)\n",
    "        \n",
    "    def train_model(self, X, y):\n",
    "        GS = GridSearchCV(\n",
    "            estimator=pl,\n",
    "            param_grid=param_grid,\n",
    "            scoring=config[\"score\"]\n",
    "        )\n",
    "        return GS\n",
    "    \n",
    "    def save_models(self):\n",
    "        hashable = frozenset(self.models.items())\n",
    "        sha = hex(hash(hashable))[-5:]\n",
    "        path = os.path.join(\"..\", config[\"models\"][\"path\"])\n",
    "        filename = f\"{self.kind}_{sha}.mdl\"\n",
    "        pickle.dump(self.models, open(os.path.join(path, filename), 'wb'))\n",
    "        \n",
    "    def load_models(self, filename=None):\n",
    "        path = os.path.join(\"..\", config[\"models\"][\"path\"])\n",
    "        if filename is None:\n",
    "            filename = most_recent_file(path, ext=\".mdl\", prefix=self.kind)\n",
    "            if filename is None:\n",
    "                raise ValueError(\"No correct models found!\")\n",
    "        else:\n",
    "            if not re.search(fr\"^{self.kind}_\", filename):\n",
    "                raise Warning(f\"File '{filename} does not seem to be having models for {self.kind}\")\n",
    "        self.models = pickle.load(open(os.path.join(path, filename), 'rb'))\n",
    "        \n",
    "    def predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "PM = PROMsModel(kind=\"hip\")\n",
    "df = PM.load_data(mode=\"train\").sample(30)\n",
    "PM.train_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(len(get_feature_names(pl)))\n",
    "get_feature_names(pl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# show results\n",
    "pd.DataFrame(GS.cv_results_)\\\n",
    "    .filter(regex=r\"^(?!.*(split|time)).*$\")\\\n",
    "    .set_index(\"rank_test_score\").sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## force plot independent of model used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class shap_force_plot:\n",
    "    def __init__(self, GS, X, feature_names=None):\n",
    "        # split pipeline, since explainer strips DataFrame before applying model\n",
    "        self.preprocess = GS.best_estimator_[:-1].transform\n",
    "        self.explainer = shap.KernelExplainer(\n",
    "            model=GS.best_estimator_[-1].predict_proba,\n",
    "            data=self.preprocess(X),\n",
    "            link=\"identity\",\n",
    "        )\n",
    "        self.feature_names = feature_names\n",
    "\n",
    "    def plot(self, X_sample, i):\n",
    "        X_preprocessed = self.preprocess(X_sample)\n",
    "        return shap.force_plot(\n",
    "            base_value=self.explainer.expected_value[i],\n",
    "            shap_values=self.explainer.shap_values(X_preprocessed)[i],\n",
    "            feature_names=self.feature_names,\n",
    "            link=\"identity\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 100\n",
    "feature_names = [s.replace(\"t0_\",\"\").replace(\"_yes\", \"\") for s in get_feature_names(GS)]\n",
    "fp = shap_force_plot(GS, X_train.sample(sample_size), feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_train.sample()\n",
    "p = [fp.plot(X, i) for i in range(3)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[display(p_) for p_ in p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:jads] *",
   "language": "python",
   "name": "conda-env-jads-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
